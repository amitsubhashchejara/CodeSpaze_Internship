{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-05T14:04:13.415049Z","iopub.execute_input":"2024-11-05T14:04:13.415779Z","iopub.status.idle":"2024-11-05T14:04:14.660402Z","shell.execute_reply.started":"2024-11-05T14:04:13.415710Z","shell.execute_reply":"2024-11-05T14:04:14.658902Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Let's import necessary packages\n\nimport torch\nfrom torchvision.datasets import MNIST\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nfrom torch.nn.functional import one_hot","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:14.663023Z","iopub.execute_input":"2024-11-05T14:04:14.664662Z","iopub.status.idle":"2024-11-05T14:04:19.964031Z","shell.execute_reply.started":"2024-11-05T14:04:14.664611Z","shell.execute_reply":"2024-11-05T14:04:19.960783Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"torch.__version__","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:19.965962Z","iopub.execute_input":"2024-11-05T14:04:19.967044Z","iopub.status.idle":"2024-11-05T14:04:19.978013Z","shell.execute_reply.started":"2024-11-05T14:04:19.966988Z","shell.execute_reply":"2024-11-05T14:04:19.975785Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'2.4.0+cpu'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Let's get the MNIST dataset ","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:19.980624Z","iopub.execute_input":"2024-11-05T14:04:19.981075Z","iopub.status.idle":"2024-11-05T14:04:19.998744Z","shell.execute_reply.started":"2024-11-05T14:04:19.981032Z","shell.execute_reply":"2024-11-05T14:04:19.996837Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\ntrain_data = MNIST(root=\"/kaggle/working/mnist-data\", train=True, download=True)\ntest_data = MNIST(root='/kaggle/working/mnist-data', train=False, download=True)\n\n\nprint(\"Training set size:\", len(train_data))\nprint(\"Test set size:\", len(test_data))","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:20.004892Z","iopub.execute_input":"2024-11-05T14:04:20.005788Z","iopub.status.idle":"2024-11-05T14:04:33.422951Z","shell.execute_reply.started":"2024-11-05T14:04:20.005729Z","shell.execute_reply":"2024-11-05T14:04:33.421650Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:01<00:00, 5022736.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/mnist-data/MNIST/raw/train-images-idx3-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 146895.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/mnist-data/MNIST/raw/train-labels-idx1-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:01<00:00, 1395394.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/mnist-data/MNIST/raw/t10k-images-idx3-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 2488313.58it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/mnist-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /kaggle/working/mnist-data/MNIST/raw\n\nTraining set size: 60000\nTest set size: 10000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Let's check the type of the images in the dataset\n\ntype(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:33.424390Z","iopub.execute_input":"2024-11-05T14:04:33.424769Z","iopub.status.idle":"2024-11-05T14:04:33.445870Z","shell.execute_reply.started":"2024-11-05T14:04:33.424729Z","shell.execute_reply":"2024-11-05T14:04:33.444647Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tuple"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# So, the image and the target is stored in a tuple, Let's check the type of the image and the target\ntype(train_data[0][0]), type(train_data[0][1]) ","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:33.447496Z","iopub.execute_input":"2024-11-05T14:04:33.447998Z","iopub.status.idle":"2024-11-05T14:04:33.456837Z","shell.execute_reply.started":"2024-11-05T14:04:33.447954Z","shell.execute_reply":"2024-11-05T14:04:33.455436Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(PIL.Image.Image, int)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# OK, great let's look at some images in the dataset and their labels\n\nfor i in np.random.randint(0, 60000, 3):\n    image, label = train_data[i]\n\n    plt.imshow(image, cmap='gray')\n    plt.title(f'Label: {label}')\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:33.458528Z","iopub.execute_input":"2024-11-05T14:04:33.459010Z","iopub.status.idle":"2024-11-05T14:04:33.964329Z","shell.execute_reply.started":"2024-11-05T14:04:33.458956Z","shell.execute_reply":"2024-11-05T14:04:33.963038Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAANGUlEQVR4nO3cXYiV9drA4Xs5poahpGiYlDpoqFSgiUYZTRFZWTSSBBWKFB6UB3ZQlmF+QBBSmZRRgoWFE0JlEShKVHYg4iRWYCiZZB9WOipqKik6ax/s/d5s35na80yz5svrAk8Wz73WvYSZn/81zlMql8vlAICI6NHRCwDQeYgCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkC3dK+ffuiVCrFiy++2GbPuXnz5iiVSrF58+Y2e07obESBTmP16tVRKpVi+/btHb1KRQwfPjxKpVKzf0aNGtXR60FERPTs6AXgQrF8+fI4ceLEeY/9+OOPsWDBgrj99ts7aCs4nyhAO6mtrW3y2HPPPRcREQ899FA7bwPN8/ERXcqZM2di4cKFcd1110X//v2jb9++cdNNN8Xnn3/+lzMvv/xyDBs2LC6++OK4+eabY+fOnU2u2b17d0yfPj0GDBgQffr0iQkTJsTHH3/8P/c5depU7N69Ow4dOtSq9/Puu+/GiBEj4oYbbmjVPLQ1UaBLOX78eKxatSpqampi6dKlsXjx4mhoaIgpU6bE119/3eT6d955J1555ZWYM2dOzJ8/P3bu3Bm33nprHDhwIK/59ttv4/rrr49du3bF008/HS+99FL07ds3amtr48MPP/zbferr62PMmDGxYsWKwu/lq6++il27dsWDDz5YeBYqxcdHdCmXXnpp7Nu3L3r16pWPzZ49O0aPHh2vvvpqvPnmm+dd//3338eePXti6NChERFxxx13xKRJk2Lp0qWxbNmyiIiYO3duXHnllfHll19G7969IyLisccei8mTJ8dTTz0V06ZNq8h7qauriwgfHdG5OCnQpVRVVWUQGhsb48iRI3H27NmYMGFC7Nixo8n1tbW1GYSIiIkTJ8akSZNiw4YNERFx5MiR+Oyzz+L++++PP/74Iw4dOhSHDh2Kw4cPx5QpU2LPnj2xf//+v9ynpqYmyuVyLF68uND7aGxsjLVr18a4ceNizJgxhWahkkSBLuftt9+Oa6+9Nvr06RMDBw6MQYMGxfr16+PYsWNNrm3uv3peddVVsW/fvoj490miXC7Hs88+G4MGDTrvz6JFiyIi4uDBg23+Hr744ovYv3+/UwKdjo+P6FLWrFkTs2bNitra2njyySdj8ODBUVVVFc8//3zs3bu38PM1NjZGRMQTTzwRU6ZMafaakSNH/qOdm1NXVxc9evSIBx54oM2fG/4JUaBLef/996O6ujrWrVsXpVIpH/+/f9X/f3v27Gny2HfffRfDhw+PiIjq6uqIiLjooovitttua/uFm3H69On44IMPoqamJi6//PJ2eU1oKR8f0aVUVVVFRES5XM7Htm3bFlu3bm32+o8++ui8nwnU19fHtm3b4s4774yIiMGDB0dNTU2sXLkyfvvttybzDQ0Nf7tPa/5L6oYNG+Lo0aM+OqJTclKg03nrrbdi48aNTR6fO3du3H333bFu3bqYNm1aTJ06NX744Yd44403YuzYsU1+Wzji3x/9TJ48OR599NE4ffp0LF++PAYOHBjz5s3La1577bWYPHlyXHPNNTF79uyorq6OAwcOxNatW+OXX36Jb7755i93ra+vj1tuuSUWLVrU4h8219XVRe/eveO+++5r0fXQnkSBTuf1119v9vFZs2bFrFmz4vfff4+VK1fGpk2bYuzYsbFmzZp47733mr1R3cyZM6NHjx6xfPnyOHjwYEycODFWrFgRQ4YMyWvGjh0b27dvjyVLlsTq1avj8OHDMXjw4Bg3blwsXLiwTd/b8ePHY/369TF16tTo379/mz43tIVS+b/P4QBc0PxMAYAkCgAkUQAgiQIASRQASKIAQGrx7yn89y0FAOh6WvIbCE4KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpZ0cv0NU88sgjhWdOnz5deGbNmjWFZ+jeHn/88cIz48ePLzwzY8aMwjMvvPBC4Zl58+YVnqHynBQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUKpfL5RZdWCpVepd2t3v37sIz1dXVhWda+Fd8nmPHjhWeiYj4888/C88sWrSo8Mxdd91VeGbcuHGFZyIi+vXr16q57mbAgAGFZ9rr6/bcuXOFZ2bOnNmq11q7dm2r5mjZ9yInBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApJ4dvUBHas0NxqqqqiqwSVMDBw5sl9eJiFi1alW7vRbdU2u+Lvr06VOBTfinnBQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAu6Bvi/fTTT4Vn2vNGddBVnDp1qvBMQ0NDBTbhn3JSACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAKpXL5XKLLiyVKr1Luxs2bFjhmfHjx7fL68yZM6fwTEREjx7FO9+vX7/CM0ePHi080x215utixIgRFdikYz3zzDOFZ5YuXVqBTfg7Lfl276QAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkC/ouqd1Rr169Cs9cffXVhWd27NhReKY76t+/f+GZI0eOVGCTtnPu3LnCM/fcc0/hmU2bNhWe4Z9xl1QAChEFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUs6MXoG2dOXOm8Iyb27XekiVLOnqFNrdly5bCM25u1304KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAILkhHnCe+fPnd/QKdCAnBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJDfEg/8YMmRI4ZmHH364Apu0nZ9//rnwTENDQwU2oatwUgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHJDPPiP0aNHF57p27dvBTZpO/X19YVn9u7dW4FN6CqcFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOQuqXRLI0eOLDxTV1dXgU061q+//trRK9DFOCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACC5IR7d0iWXXFJ45rLLLqvAJm3n008/LTyzYMGCCmxCd+akAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5IZ4dEv33ntvR6/Q5pYtW1Z45sSJExXYhO7MSQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAMkN8eiWbrzxxo5eoc1t2bKlo1fgAuCkAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5IZ4dHpXXHFF4ZmhQ4dWYBPo/pwUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5C6pdHrTp08vPDN69OgKbNI2VqxY0aq5kydPtvEm0JSTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkhvi0enNmDGjo1doU0ePHm3VXGNjY9suAs1wUgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg9ezoBeB/OXnyZEevABcMJwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACC5Syqd3owZMwrPbNy4sfDMqFGjCs+cPXu28Mwnn3xSeAbai5MCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSqVwul1t0YalU6V0AqKCWfLt3UgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQOrZ0gtbeN88ALowJwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0r8A6SQ3G291wyUAAAAASUVORK5CYII="},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Let's define a transform to convert images to tensors and normalize pixel values to [0, 1]\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:33.965778Z","iopub.execute_input":"2024-11-05T14:04:33.966249Z","iopub.status.idle":"2024-11-05T14:04:33.972720Z","shell.execute_reply.started":"2024-11-05T14:04:33.966199Z","shell.execute_reply":"2024-11-05T14:04:33.971315Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_data = MNIST(root='/kaggle/working/mnist-data', train=True, transform=ToTensor(), download=False)\ntest_data = MNIST(root='/kaggle/working/mnist-data', train=False, transform=ToTensor(), download=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:33.974462Z","iopub.execute_input":"2024-11-05T14:04:33.975051Z","iopub.status.idle":"2024-11-05T14:04:34.087687Z","shell.execute_reply.started":"2024-11-05T14:04:33.974999Z","shell.execute_reply":"2024-11-05T14:04:34.086721Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Now let's have a look at the the data\ntrain_data[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:34.089196Z","iopub.execute_input":"2024-11-05T14:04:34.089670Z","iopub.status.idle":"2024-11-05T14:04:34.150857Z","shell.execute_reply.started":"2024-11-05T14:04:34.089621Z","shell.execute_reply":"2024-11-05T14:04:34.149555Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000]]])"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Great, this is a tensor, let's check it shape\ntrain_data[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:34.152771Z","iopub.execute_input":"2024-11-05T14:04:34.153277Z","iopub.status.idle":"2024-11-05T14:04:34.163402Z","shell.execute_reply.started":"2024-11-05T14:04:34.153222Z","shell.execute_reply":"2024-11-05T14:04:34.161998Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 28, 28])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Let's seperate the cross validation set aside\n\nfrom torch.utils.data import random_split\n\n\ncv_size = int(0.5 * len(test_data))  # 50% for CV\ntest_size = len(test_data) - cv_size  # Remaining 50% for Test\n\ncv_data, test_data = random_split(test_data, [cv_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:34.164928Z","iopub.execute_input":"2024-11-05T14:04:34.165349Z","iopub.status.idle":"2024-11-05T14:04:34.182092Z","shell.execute_reply.started":"2024-11-05T14:04:34.165307Z","shell.execute_reply":"2024-11-05T14:04:34.180838Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Now let's seperate the training instance and the target from the tuple and create a 2d tensor of shape (number of instances, 1\\*28\\*28) containing all the instances and a 2d tensor of shape (number of instances, number of classes) containing one hot encoded version of all the targets for both train and the test set.","metadata":{}},{"cell_type":"code","source":"def transform_shape(data_set):\n    \n    data_examples = torch.zeros(size=(len(data_set), 1*28*28))\n    targets = torch.zeros(size=(len(data_set), 1))\n    \n    for i,instance in enumerate(data_set):\n        data_examples[i] = instance[0].reshape(1*28*28) \n        targets[i] = instance[1]\n        \n    targets = one_hot(targets.long(), 10).squeeze()\n        \n    return data_examples, targets\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:34.187584Z","iopub.execute_input":"2024-11-05T14:04:34.187977Z","iopub.status.idle":"2024-11-05T14:04:34.195608Z","shell.execute_reply.started":"2024-11-05T14:04:34.187939Z","shell.execute_reply":"2024-11-05T14:04:34.194210Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"new_train_x, new_train_y = transform_shape(train_data)\nnew_test_x, new_test_y = transform_shape(test_data)\nnew_cv_x, new_cv_y = transform_shape(cv_data)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:34.197113Z","iopub.execute_input":"2024-11-05T14:04:34.197627Z","iopub.status.idle":"2024-11-05T14:04:43.439338Z","shell.execute_reply.started":"2024-11-05T14:04:34.197567Z","shell.execute_reply":"2024-11-05T14:04:43.438061Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Let's check thier shape\nnew_train_x.shape, new_train_y.shape, new_test_x.shape, new_test_y.shape, new_cv_x.shape, new_cv_y.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.440783Z","iopub.execute_input":"2024-11-05T14:04:43.441207Z","iopub.status.idle":"2024-11-05T14:04:43.449073Z","shell.execute_reply.started":"2024-11-05T14:04:43.441137Z","shell.execute_reply":"2024-11-05T14:04:43.447698Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(torch.Size([60000, 784]),\n torch.Size([60000, 10]),\n torch.Size([5000, 784]),\n torch.Size([5000, 10]),\n torch.Size([5000, 784]),\n torch.Size([5000, 10]))"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Great, the data transformation was successful and the shapes look good, let's check one instance and its target\n\nnew_train_x[0]","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.451033Z","iopub.execute_input":"2024-11-05T14:04:43.451523Z","iopub.status.idle":"2024-11-05T14:04:43.463868Z","shell.execute_reply.started":"2024-11-05T14:04:43.451483Z","shell.execute_reply":"2024-11-05T14:04:43.462619Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n        0.0706, 0.0706, 0.0706, 0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000,\n        0.9686, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.1412, 0.3686, 0.6039,\n        0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8824, 0.6745, 0.9922,\n        0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333, 0.9922, 0.9922,\n        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843, 0.3647, 0.3216,\n        0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588, 0.9922,\n        0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137,\n        0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451,\n        0.8824, 0.6275, 0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.3176, 0.9412, 0.9922, 0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922, 0.5882, 0.1059, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882, 0.9922, 0.7333,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922,\n        0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922,\n        0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922,\n        0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n        0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922,\n        0.7765, 0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0706, 0.6706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647,\n        0.3137, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.2157, 0.6745, 0.8863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9569, 0.5216,\n        0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314, 0.5294, 0.5176, 0.0627,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"new_train_y[0]","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.465903Z","iopub.execute_input":"2024-11-05T14:04:43.466431Z","iopub.status.idle":"2024-11-05T14:04:43.475833Z","shell.execute_reply.started":"2024-11-05T14:04:43.466372Z","shell.execute_reply":"2024-11-05T14:04:43.474267Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# The data is ready, let's build a Neural Network 😊","metadata":{}},{"cell_type":"code","source":"# Let's import some other necessary modules\nfrom torch import nn\nfrom torch.nn import Sequential\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import LambdaLR","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.477569Z","iopub.execute_input":"2024-11-05T14:04:43.477971Z","iopub.status.idle":"2024-11-05T14:04:43.484711Z","shell.execute_reply.started":"2024-11-05T14:04:43.477931Z","shell.execute_reply":"2024-11-05T14:04:43.483257Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Make device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.488775Z","iopub.execute_input":"2024-11-05T14:04:43.489874Z","iopub.status.idle":"2024-11-05T14:04:43.499719Z","shell.execute_reply.started":"2024-11-05T14:04:43.489825Z","shell.execute_reply":"2024-11-05T14:04:43.498450Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"my_model = Sequential(\n    nn.Linear(new_train_x[0].shape[0],256),\n    nn.ReLU(),\n    nn.Linear(256,128),\n    nn.ReLU(),\n    nn.Linear(128,64),\n    nn.ReLU(),\n    nn.Linear(64,10),\n    nn.Softmax(dim=1)\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.501130Z","iopub.execute_input":"2024-11-05T14:04:43.501676Z","iopub.status.idle":"2024-11-05T14:04:43.517213Z","shell.execute_reply.started":"2024-11-05T14:04:43.501565Z","shell.execute_reply":"2024-11-05T14:04:43.515980Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Let's looka at parameters of our model\n\nfor name, param in my_model.named_parameters():\n    print(f\"Parameter Name: {name}\")\n    print(f\"Shape: {param.shape}\")\n    print(param)\n    print(\"-\" * 50)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.518679Z","iopub.execute_input":"2024-11-05T14:04:43.519079Z","iopub.status.idle":"2024-11-05T14:04:43.554192Z","shell.execute_reply.started":"2024-11-05T14:04:43.519039Z","shell.execute_reply":"2024-11-05T14:04:43.552375Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Parameter Name: 0.weight\nShape: torch.Size([256, 784])\nParameter containing:\ntensor([[ 0.0185, -0.0123, -0.0186,  ...,  0.0145,  0.0119,  0.0208],\n        [-0.0316,  0.0156,  0.0143,  ..., -0.0163, -0.0280, -0.0228],\n        [-0.0324, -0.0011, -0.0244,  ...,  0.0083,  0.0080,  0.0009],\n        ...,\n        [-0.0128, -0.0282, -0.0222,  ...,  0.0151, -0.0328, -0.0321],\n        [-0.0134, -0.0148, -0.0239,  ...,  0.0295,  0.0105,  0.0332],\n        [ 0.0058,  0.0208,  0.0203,  ...,  0.0131, -0.0254, -0.0111]],\n       requires_grad=True)\n--------------------------------------------------\nParameter Name: 0.bias\nShape: torch.Size([256])\nParameter containing:\ntensor([ 0.0283, -0.0259, -0.0341,  0.0146, -0.0283,  0.0245,  0.0158,  0.0330,\n         0.0156, -0.0007, -0.0177,  0.0348,  0.0294, -0.0058, -0.0338,  0.0098,\n        -0.0180,  0.0210,  0.0191,  0.0114, -0.0180,  0.0179,  0.0245, -0.0053,\n         0.0316,  0.0214, -0.0167,  0.0071,  0.0274, -0.0073,  0.0174,  0.0300,\n        -0.0184,  0.0313, -0.0041, -0.0009,  0.0355, -0.0238, -0.0267,  0.0063,\n        -0.0320,  0.0068,  0.0349,  0.0183,  0.0328,  0.0344,  0.0235,  0.0246,\n         0.0117,  0.0024, -0.0356, -0.0151,  0.0184,  0.0203, -0.0151, -0.0253,\n         0.0347,  0.0199, -0.0095, -0.0002, -0.0009,  0.0236,  0.0195,  0.0033,\n        -0.0017, -0.0222, -0.0289, -0.0023, -0.0327,  0.0031,  0.0201, -0.0137,\n         0.0201, -0.0143, -0.0144,  0.0168,  0.0173,  0.0351,  0.0296,  0.0066,\n        -0.0290, -0.0132,  0.0180, -0.0144, -0.0121, -0.0005,  0.0348, -0.0125,\n        -0.0103,  0.0024, -0.0195,  0.0041,  0.0247, -0.0236,  0.0117, -0.0203,\n         0.0277,  0.0267,  0.0326, -0.0065,  0.0290,  0.0323, -0.0173,  0.0245,\n         0.0324, -0.0098, -0.0192,  0.0098, -0.0133,  0.0142, -0.0036,  0.0312,\n        -0.0115, -0.0295, -0.0136, -0.0174, -0.0006, -0.0344,  0.0087,  0.0043,\n        -0.0252,  0.0291,  0.0247,  0.0279,  0.0173,  0.0194,  0.0091,  0.0007,\n         0.0287,  0.0102, -0.0057,  0.0236, -0.0034, -0.0111, -0.0026, -0.0345,\n        -0.0150,  0.0354, -0.0269,  0.0032, -0.0120, -0.0080,  0.0187, -0.0138,\n        -0.0087, -0.0242, -0.0008, -0.0210, -0.0087, -0.0082, -0.0066,  0.0057,\n        -0.0077,  0.0242, -0.0302, -0.0034,  0.0287,  0.0092, -0.0213, -0.0126,\n         0.0268, -0.0176,  0.0233,  0.0133,  0.0157,  0.0344, -0.0111, -0.0262,\n         0.0130,  0.0275, -0.0008,  0.0116, -0.0158,  0.0250, -0.0122, -0.0001,\n        -0.0241, -0.0349,  0.0254, -0.0336, -0.0292, -0.0054,  0.0301,  0.0208,\n        -0.0251, -0.0054,  0.0124,  0.0175,  0.0115, -0.0266,  0.0143, -0.0040,\n         0.0031,  0.0293,  0.0173, -0.0186, -0.0068, -0.0259, -0.0216, -0.0269,\n         0.0270,  0.0078, -0.0061,  0.0170, -0.0233,  0.0071, -0.0002, -0.0191,\n        -0.0200, -0.0277, -0.0294,  0.0279,  0.0091,  0.0237,  0.0040,  0.0016,\n         0.0188,  0.0094,  0.0248,  0.0167, -0.0122, -0.0225,  0.0311, -0.0307,\n         0.0339,  0.0179, -0.0176, -0.0089,  0.0263, -0.0143, -0.0072, -0.0021,\n         0.0104, -0.0186,  0.0315, -0.0276,  0.0209, -0.0064, -0.0079, -0.0258,\n         0.0184,  0.0035, -0.0099, -0.0226,  0.0174, -0.0216,  0.0308,  0.0196,\n         0.0065,  0.0175,  0.0057,  0.0349,  0.0013, -0.0131,  0.0145, -0.0041],\n       requires_grad=True)\n--------------------------------------------------\nParameter Name: 2.weight\nShape: torch.Size([128, 256])\nParameter containing:\ntensor([[-5.7007e-02, -2.1244e-02, -1.4391e-02,  ...,  3.4029e-02,\n          1.2265e-03,  1.6897e-02],\n        [-2.1785e-02,  3.2605e-02, -2.0176e-02,  ..., -4.4805e-02,\n          3.5093e-02, -1.4723e-02],\n        [-2.5816e-02,  4.8146e-02, -2.6805e-02,  ...,  2.1574e-02,\n          6.1151e-02,  6.9257e-03],\n        ...,\n        [-1.1419e-02, -3.2611e-02, -5.9159e-02,  ...,  2.0880e-02,\n         -4.6446e-02, -2.2663e-03],\n        [ 5.9800e-02, -4.0807e-05,  2.0859e-02,  ..., -4.3608e-02,\n          5.8952e-02, -5.3166e-02],\n        [ 3.7047e-03,  1.8268e-02,  3.2410e-02,  ...,  2.3678e-02,\n         -2.5381e-02, -5.4317e-03]], requires_grad=True)\n--------------------------------------------------\nParameter Name: 2.bias\nShape: torch.Size([128])\nParameter containing:\ntensor([ 0.0348,  0.0538,  0.0264, -0.0116, -0.0086,  0.0110, -0.0464, -0.0048,\n         0.0484,  0.0123,  0.0607,  0.0538,  0.0034,  0.0349,  0.0495,  0.0028,\n        -0.0016, -0.0565,  0.0269, -0.0299, -0.0508,  0.0516,  0.0431, -0.0564,\n         0.0522,  0.0496,  0.0131, -0.0405, -0.0442,  0.0474, -0.0461, -0.0371,\n         0.0093, -0.0011,  0.0397, -0.0017,  0.0421, -0.0362,  0.0189,  0.0615,\n         0.0268,  0.0006, -0.0035, -0.0178,  0.0082, -0.0265, -0.0370, -0.0314,\n         0.0007,  0.0360, -0.0324, -0.0546, -0.0506,  0.0501,  0.0590,  0.0254,\n        -0.0553, -0.0556,  0.0311, -0.0574,  0.0080,  0.0441,  0.0227,  0.0203,\n         0.0482, -0.0155, -0.0143,  0.0420, -0.0588, -0.0073,  0.0292,  0.0556,\n         0.0557, -0.0204,  0.0111,  0.0395,  0.0446, -0.0317,  0.0293,  0.0192,\n        -0.0334,  0.0595, -0.0070, -0.0419, -0.0516, -0.0086,  0.0552, -0.0225,\n         0.0037,  0.0030,  0.0053,  0.0368, -0.0254, -0.0062, -0.0548,  0.0055,\n        -0.0543, -0.0510, -0.0071, -0.0331, -0.0487, -0.0368, -0.0050,  0.0192,\n         0.0504,  0.0029, -0.0251, -0.0149,  0.0188, -0.0358,  0.0155, -0.0584,\n         0.0267, -0.0330,  0.0284,  0.0477,  0.0030,  0.0359, -0.0310, -0.0382,\n        -0.0524, -0.0521, -0.0011,  0.0077,  0.0271,  0.0347, -0.0594,  0.0183],\n       requires_grad=True)\n--------------------------------------------------\nParameter Name: 4.weight\nShape: torch.Size([64, 128])\nParameter containing:\ntensor([[-0.0405, -0.0035, -0.0051,  ...,  0.0437,  0.0402,  0.0449],\n        [-0.0536,  0.0869,  0.0380,  ...,  0.0884,  0.0301, -0.0528],\n        [ 0.0211,  0.0039, -0.0378,  ...,  0.0453, -0.0698,  0.0806],\n        ...,\n        [-0.0423,  0.0286,  0.0786,  ..., -0.0043,  0.0494,  0.0254],\n        [-0.0205,  0.0120, -0.0415,  ...,  0.0691,  0.0482, -0.0043],\n        [-0.0390,  0.0064,  0.0768,  ...,  0.0096,  0.0395, -0.0676]],\n       requires_grad=True)\n--------------------------------------------------\nParameter Name: 4.bias\nShape: torch.Size([64])\nParameter containing:\ntensor([-8.3368e-02, -7.4521e-02, -1.8711e-03, -4.0210e-04,  1.1549e-02,\n        -1.8802e-02,  2.6518e-02,  2.0206e-02, -1.1742e-02, -4.1749e-02,\n         6.7061e-02, -4.7591e-02,  6.1094e-02,  2.7957e-02,  2.7709e-02,\n        -4.9646e-02,  7.1860e-02,  8.1197e-02, -3.2865e-02,  5.7303e-02,\n        -5.1785e-02, -7.9039e-03, -2.9598e-02, -7.3652e-06,  5.2128e-03,\n        -7.2768e-02,  7.3199e-02, -4.1354e-02,  3.1780e-02, -5.9819e-02,\n         1.6485e-02,  4.6934e-02, -8.4741e-02,  8.9116e-03,  4.3401e-02,\n        -6.3367e-02,  3.1180e-02, -6.6137e-02, -3.9551e-02, -2.6589e-02,\n        -7.0366e-02, -6.1785e-02,  7.1527e-02,  1.3990e-02, -2.2604e-03,\n         1.8885e-02,  3.7188e-02, -1.7230e-02,  2.8565e-03, -7.2096e-02,\n         8.1828e-02,  2.1960e-02,  3.7214e-02,  2.8455e-02, -3.8550e-02,\n        -8.1376e-02, -4.7096e-02, -2.7169e-02, -1.7795e-02,  6.5466e-02,\n        -2.7881e-02,  2.7162e-03,  7.3318e-03,  2.2175e-02],\n       requires_grad=True)\n--------------------------------------------------\nParameter Name: 6.weight\nShape: torch.Size([10, 64])\nParameter containing:\ntensor([[-8.0389e-02, -6.8235e-02,  5.2144e-02, -5.1750e-02, -2.1274e-02,\n          2.8855e-02, -6.4110e-03,  2.3624e-03,  3.9338e-02, -4.0579e-02,\n         -1.7403e-02,  7.3138e-02, -4.1517e-02,  6.9714e-02,  1.1148e-01,\n         -1.1007e-01, -1.8476e-02,  4.5115e-02,  1.0545e-01,  3.0383e-02,\n          1.0021e-01, -1.2425e-01, -8.7993e-02, -5.7451e-02, -1.1871e-01,\n         -5.7619e-02, -6.3153e-02,  8.5909e-02, -8.2416e-02, -3.0745e-02,\n          1.2146e-02, -9.5195e-02,  8.3489e-02,  8.9038e-02, -9.7982e-02,\n         -5.4434e-02, -8.4639e-02,  3.0636e-02,  8.8097e-02, -1.9401e-03,\n         -1.2157e-01,  1.0395e-03, -8.1736e-02, -6.4782e-02,  3.8503e-02,\n         -1.0004e-01, -1.1802e-01, -1.0331e-01,  5.2904e-02,  1.1037e-02,\n         -1.2249e-01, -3.3824e-02, -1.0919e-01, -1.1122e-01,  1.2257e-01,\n          1.0570e-01, -8.2738e-02,  1.0911e-01, -2.6759e-02, -9.6898e-02,\n          5.3173e-02,  1.4375e-02, -6.2388e-02, -3.9966e-02],\n        [ 6.3548e-02, -8.6374e-03, -7.1923e-02,  9.2590e-02, -4.4983e-02,\n         -7.5485e-02, -1.1885e-01, -4.2094e-03,  5.6329e-02,  1.1654e-01,\n          9.0605e-02,  1.5003e-02,  7.9350e-02, -7.3008e-02, -8.6639e-02,\n         -1.0280e-01, -2.0855e-02, -1.1383e-01,  2.0806e-02, -2.9670e-02,\n          5.6077e-02, -8.3940e-02, -6.8961e-02, -1.0767e-01,  9.2662e-02,\n         -8.9517e-02,  1.2120e-01,  6.2972e-02,  9.2726e-02, -8.2972e-02,\n          7.9286e-02,  4.3577e-02,  1.0551e-01, -1.2232e-01,  1.2479e-01,\n          1.8101e-02,  3.4121e-02, -4.9534e-02, -9.7167e-02,  6.8020e-02,\n          7.5072e-03,  4.0372e-02,  3.3493e-02,  1.0446e-01, -6.4614e-02,\n         -6.6064e-02, -5.6316e-02,  9.3602e-03, -8.1036e-02, -5.4569e-02,\n         -7.3994e-02,  9.7457e-02,  2.8583e-02,  3.4644e-02, -5.5054e-02,\n          9.8586e-02, -8.3562e-02,  7.2699e-02, -3.2836e-02, -1.2077e-01,\n         -6.3948e-02, -1.0502e-01,  1.0243e-01, -4.6757e-02],\n        [-1.2820e-02, -7.1798e-02,  1.0048e-01,  8.4844e-04,  1.0430e-01,\n          3.8612e-02, -1.2160e-01,  1.0987e-01, -3.6614e-03, -2.9908e-02,\n         -1.1321e-01,  1.9816e-02, -1.8553e-02, -6.2801e-02,  7.3899e-02,\n          1.2356e-02,  1.9114e-02,  1.0447e-01, -1.0572e-01,  3.8152e-02,\n         -1.2106e-01, -3.2695e-02,  8.4840e-02,  5.8548e-02, -4.4885e-03,\n         -1.2407e-01, -5.7780e-02,  1.2457e-02,  8.6854e-02, -5.9139e-02,\n         -1.9258e-02,  8.5183e-02, -2.1680e-02, -6.7002e-02,  9.5725e-02,\n          4.2999e-03, -3.5930e-02,  8.2936e-02,  4.0338e-02, -9.4790e-02,\n         -8.5625e-02,  2.1363e-03, -1.0080e-01, -1.7248e-02,  2.8347e-02,\n          4.5453e-02, -1.0510e-02, -4.7141e-02, -1.2409e-01,  2.8854e-02,\n         -8.4876e-02,  8.0641e-02,  4.5339e-02, -6.5984e-02, -8.8194e-02,\n         -1.2454e-01, -1.1397e-01, -8.0481e-02, -8.9662e-02,  1.5703e-02,\n         -1.0148e-01,  3.0439e-02,  7.9503e-02,  2.0556e-02],\n        [ 1.2651e-02,  8.5369e-02, -2.5288e-02, -8.3660e-02, -1.9862e-02,\n          5.2645e-02, -1.0882e-01,  1.7326e-02, -1.1848e-01, -3.6025e-02,\n         -4.3525e-03,  1.1211e-01, -3.0908e-04, -8.9750e-05,  8.7689e-02,\n          4.7603e-02,  1.1232e-01, -1.1087e-01,  6.1689e-02, -5.7920e-02,\n         -4.7656e-02, -5.8721e-02, -3.6528e-02, -9.9206e-02, -3.2899e-02,\n          5.2357e-02,  8.5284e-02, -4.0644e-02, -1.2176e-01, -1.0526e-02,\n          7.3309e-02, -7.8929e-02,  1.2315e-01, -8.9100e-02,  1.1613e-01,\n          8.9340e-03,  7.6948e-02,  9.1863e-02,  8.1436e-02, -1.1504e-01,\n         -7.1399e-02,  1.5884e-02,  4.6399e-02,  6.3031e-02, -6.4139e-02,\n         -2.1684e-02, -3.9519e-03, -9.0992e-02, -7.2082e-02, -3.3272e-02,\n         -1.3969e-02,  9.7112e-02, -1.0904e-01,  8.4963e-02, -1.0228e-01,\n         -8.4851e-02,  1.0560e-01, -1.1197e-01, -1.2065e-01, -8.5090e-02,\n          9.2944e-02, -8.0306e-02,  7.9992e-02, -6.4797e-02],\n        [-2.8603e-02,  5.0266e-02, -3.1099e-02, -1.0440e-01, -3.2069e-02,\n         -1.1828e-01,  1.4381e-02, -3.7380e-02, -2.0239e-02,  6.7794e-02,\n         -1.0331e-02, -2.7107e-02, -8.5706e-03,  1.2411e-01,  1.0935e-01,\n          4.9039e-02, -1.1346e-01,  7.8490e-02, -1.3764e-02,  1.7386e-02,\n          9.4693e-02, -7.3676e-02,  2.0655e-02,  8.8837e-02,  3.6262e-04,\n         -7.5166e-02,  1.0308e-01,  9.5862e-02,  5.7069e-02, -1.5022e-02,\n         -4.1769e-02,  1.2412e-02, -7.2238e-02, -8.4503e-02,  8.4332e-02,\n         -9.8308e-02,  7.8189e-02, -1.1794e-01,  8.4894e-03, -7.5132e-02,\n          7.3697e-02, -7.1773e-02, -1.0917e-02, -6.2549e-02, -2.4981e-02,\n          6.7132e-03, -1.1376e-01,  4.0632e-03, -1.2438e-01, -1.1229e-01,\n          1.1745e-01,  4.6345e-02,  3.9821e-02,  1.1139e-02,  1.7358e-02,\n          1.2176e-01, -1.0339e-01, -5.4668e-02,  1.2177e-01, -1.1129e-01,\n          6.4445e-02, -3.4268e-02,  7.8405e-02,  1.1996e-01],\n        [ 7.5726e-02,  4.4456e-02, -6.3194e-02, -1.1851e-01, -2.2801e-02,\n         -3.9087e-02, -3.1642e-02, -3.0627e-02,  1.1916e-01, -1.2360e-01,\n          1.0613e-01, -1.2811e-02, -9.7331e-02, -5.5818e-02,  1.1835e-02,\n         -1.0350e-01, -2.8814e-02,  1.1257e-01, -9.9157e-04, -4.8831e-02,\n         -4.8827e-02,  6.8401e-02,  6.9994e-02, -9.1555e-03,  1.5037e-02,\n          1.1478e-01,  6.1819e-02, -4.4767e-02,  1.2060e-01,  8.0972e-02,\n         -5.4292e-02,  2.0242e-02,  2.3634e-02,  7.0343e-02, -2.4624e-02,\n         -4.1549e-02, -4.5813e-02,  2.5840e-02,  1.0791e-01,  1.2348e-01,\n         -9.7900e-02,  1.1158e-01, -4.7354e-03,  1.0703e-01,  8.0083e-02,\n         -2.7885e-02, -8.0856e-02, -9.1503e-02,  5.7174e-02, -8.1433e-02,\n          7.0372e-02,  7.4376e-02,  3.6002e-02,  2.9546e-02, -5.4223e-02,\n         -4.5820e-02,  4.3524e-02, -8.7596e-02,  1.8246e-03,  3.7969e-03,\n         -2.5913e-02,  5.0523e-02, -1.6923e-02, -2.1614e-02],\n        [ 3.0513e-02,  5.0270e-02,  9.5447e-02,  7.4488e-02, -7.2096e-02,\n         -8.5723e-02,  1.1827e-01, -1.2007e-01, -1.0790e-01, -1.0959e-01,\n         -4.6113e-02, -8.2213e-02, -3.7593e-02, -5.6441e-02,  1.0803e-02,\n          1.2794e-02, -4.4002e-02,  5.4201e-02,  1.1882e-02,  9.6496e-02,\n         -7.4569e-02, -9.1479e-02, -5.6442e-02, -4.7967e-02, -7.3286e-02,\n         -1.2087e-01, -3.7054e-02, -5.7539e-02,  1.0382e-02, -1.2075e-01,\n         -1.0034e-01,  2.3819e-02, -2.1634e-02, -5.2596e-03, -1.7413e-02,\n          9.1820e-02, -1.1732e-01,  6.9642e-02,  1.1727e-01,  1.1221e-02,\n          1.1007e-01, -3.7662e-02, -7.6247e-02,  1.0693e-01,  5.6005e-02,\n         -2.1674e-02, -3.2625e-02,  4.9066e-02,  5.6022e-02,  1.5276e-02,\n          1.0359e-01,  7.8521e-02, -8.8541e-03,  5.7763e-02,  6.5119e-02,\n         -1.1363e-01, -1.5180e-02, -6.6831e-02, -9.3191e-02,  1.1364e-01,\n         -1.1425e-01, -1.3025e-02,  5.1294e-03,  2.0365e-02],\n        [-9.9482e-03,  4.5639e-02, -9.8590e-02,  6.6187e-02, -2.5463e-02,\n         -7.2744e-03,  1.0730e-01,  4.6262e-02, -6.2366e-03, -2.4447e-02,\n          7.6883e-02, -6.6726e-02,  1.1855e-01,  1.1036e-01, -2.0790e-02,\n         -2.1958e-03, -2.6286e-02,  6.2768e-02,  9.4613e-02,  9.2268e-02,\n          1.2341e-01, -1.0412e-01,  8.1245e-02, -5.6358e-04,  1.0918e-01,\n         -1.1799e-01, -8.8753e-02, -1.0832e-01,  1.0097e-01,  1.0314e-01,\n          9.1502e-02, -9.5680e-02, -2.5161e-02,  1.9350e-02, -1.6548e-02,\n          2.5049e-02,  6.6014e-02, -4.0307e-02,  3.0286e-02,  1.0009e-01,\n          3.4644e-02, -3.1800e-02,  3.8460e-02,  6.6381e-02, -1.1582e-01,\n          1.1477e-01, -1.0721e-01, -7.7255e-02, -1.5223e-02, -2.7143e-02,\n          6.0805e-02, -8.6232e-02,  4.7663e-02, -2.5820e-03,  1.9759e-02,\n         -9.7544e-02,  4.7714e-02, -2.1688e-02, -8.2696e-02,  3.8758e-02,\n          6.7734e-02,  3.3035e-02,  9.0852e-02,  1.1865e-01],\n        [ 1.1806e-01, -7.8383e-02,  6.1277e-03, -7.8625e-02,  6.4668e-02,\n          4.1224e-02,  1.0695e-01, -7.7263e-02,  2.5754e-02, -1.0345e-01,\n          8.3600e-02, -7.6905e-02, -2.4964e-02, -3.9412e-02,  1.1245e-02,\n         -6.2508e-02, -6.1857e-03,  8.3546e-02, -5.4718e-02, -1.1243e-01,\n         -2.2281e-02, -8.0946e-02, -2.8875e-02,  1.0753e-01,  6.3193e-02,\n         -1.3609e-02,  1.0278e-01, -1.4099e-03,  5.3114e-02, -1.0079e-01,\n         -7.6636e-02,  1.2109e-01, -7.4255e-04, -1.1205e-01,  1.2510e-02,\n         -3.8074e-02,  3.4992e-02, -3.4718e-02, -6.5316e-02, -4.7020e-02,\n          6.3593e-02,  3.0433e-02, -6.9729e-02, -7.4933e-03,  4.6881e-02,\n          1.0963e-01, -6.9336e-02,  7.0538e-02, -6.5541e-03, -9.1975e-02,\n         -8.0136e-02,  7.9175e-02,  9.1690e-02, -9.4410e-02, -1.0081e-01,\n         -4.7719e-02, -3.0632e-02,  9.9466e-02, -5.1354e-02,  4.5308e-02,\n         -6.9018e-04,  8.3029e-02, -8.7577e-02,  9.7643e-02],\n        [-1.1487e-02,  6.2948e-02, -1.0639e-01, -2.7538e-02,  2.0762e-02,\n          9.4789e-03, -1.2385e-01,  3.1745e-02, -9.7924e-02,  5.1791e-02,\n         -9.8150e-02,  3.6760e-02, -2.8886e-02,  6.6632e-04, -8.9935e-02,\n          1.9931e-02, -4.8774e-02, -3.5465e-03,  2.3130e-02,  9.1314e-02,\n         -1.2226e-02,  2.0380e-02,  3.4242e-02,  5.0512e-02, -1.1697e-01,\n          1.1425e-01, -6.2780e-02,  6.2902e-02,  8.4657e-03, -4.7486e-02,\n          4.9165e-03,  8.5782e-02,  2.6972e-02, -4.0820e-02,  3.2307e-02,\n          1.2442e-01, -7.2155e-02, -1.2321e-02, -5.7154e-02,  5.4528e-02,\n         -6.5009e-02, -5.5954e-02,  4.5314e-02,  9.2414e-02, -2.9938e-02,\n          6.6872e-02, -6.0490e-02, -4.6544e-02, -3.3104e-02, -1.1161e-01,\n          3.7787e-02,  7.2104e-02,  1.2154e-01, -1.1830e-01,  7.0585e-03,\n          9.1242e-02, -1.6987e-03, -1.3684e-02,  1.2426e-01,  4.1800e-02,\n          4.7475e-02,  7.4556e-02,  1.0625e-01, -6.3307e-02]],\n       requires_grad=True)\n--------------------------------------------------\nParameter Name: 6.bias\nShape: torch.Size([10])\nParameter containing:\ntensor([ 0.1140,  0.1070, -0.0076,  0.1040,  0.0488,  0.0234, -0.0939, -0.0774,\n        -0.0334, -0.0919], requires_grad=True)\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Now we've got a model, let's see what happens when we pass some data through it.\n\nuntrained_preds = my_model(new_cv_x.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(new_cv_y)}, Shape: {new_cv_y.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\nprint(f\"\\nFirst 10 test labels:\\n{new_cv_y[:10]}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.555810Z","iopub.execute_input":"2024-11-05T14:04:43.556254Z","iopub.status.idle":"2024-11-05T14:04:43.627554Z","shell.execute_reply.started":"2024-11-05T14:04:43.556197Z","shell.execute_reply":"2024-11-05T14:04:43.626054Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Length of predictions: 5000, Shape: torch.Size([5000, 10])\nLength of test samples: 5000, Shape: torch.Size([5000, 10])\n\nFirst 10 predictions:\ntensor([[0.1062, 0.1091, 0.0984, 0.1104, 0.1054, 0.1014, 0.0899, 0.0931, 0.0967,\n         0.0893],\n        [0.1054, 0.1088, 0.0981, 0.1085, 0.1044, 0.1034, 0.0914, 0.0947, 0.0965,\n         0.0888],\n        [0.1051, 0.1087, 0.0988, 0.1088, 0.1068, 0.1031, 0.0888, 0.0929, 0.0984,\n         0.0886],\n        [0.1073, 0.1092, 0.0979, 0.1096, 0.1052, 0.1019, 0.0900, 0.0935, 0.0968,\n         0.0886],\n        [0.1062, 0.1082, 0.0977, 0.1094, 0.1057, 0.1026, 0.0897, 0.0944, 0.0969,\n         0.0893],\n        [0.1062, 0.1083, 0.0988, 0.1098, 0.1052, 0.1024, 0.0904, 0.0937, 0.0960,\n         0.0893],\n        [0.1065, 0.1091, 0.0975, 0.1099, 0.1055, 0.1021, 0.0893, 0.0944, 0.0966,\n         0.0890],\n        [0.1075, 0.1085, 0.0985, 0.1095, 0.1048, 0.1014, 0.0907, 0.0936, 0.0961,\n         0.0894],\n        [0.1062, 0.1085, 0.0985, 0.1097, 0.1053, 0.1023, 0.0901, 0.0934, 0.0967,\n         0.0892],\n        [0.1057, 0.1074, 0.0983, 0.1088, 0.1068, 0.1028, 0.0897, 0.0936, 0.0983,\n         0.0885]], grad_fn=<SliceBackward0>)\n\nFirst 10 test labels:\ntensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Let's round the predictions and see again \n\nuntrained_preds = my_model(new_cv_x.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(new_cv_y)}, Shape: {new_cv_y.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{torch.round(untrained_preds[:10])}\")\nprint(f\"\\nFirst 10 test labels:\\n{new_cv_y[:10]}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.629147Z","iopub.execute_input":"2024-11-05T14:04:43.629588Z","iopub.status.idle":"2024-11-05T14:04:43.681690Z","shell.execute_reply.started":"2024-11-05T14:04:43.629544Z","shell.execute_reply":"2024-11-05T14:04:43.679360Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Length of predictions: 5000, Shape: torch.Size([5000, 10])\nLength of test samples: 5000, Shape: torch.Size([5000, 10])\n\nFirst 10 predictions:\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<RoundBackward0>)\n\nFirst 10 test labels:\ntensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Let's setup loss function and optimizer\n\n# We will use the crossentropyloss since we are performing softmax regression\n# And we will use the adams optimizer.\n\nloss = nn.CrossEntropyLoss()\noptimizer = Adam(params = my_model.parameters(), lr = 0.01)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.683344Z","iopub.execute_input":"2024-11-05T14:04:43.683876Z","iopub.status.idle":"2024-11-05T14:04:43.692407Z","shell.execute_reply.started":"2024-11-05T14:04:43.683822Z","shell.execute_reply":"2024-11-05T14:04:43.690275Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.all(y_true == y_pred, dim=1).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.694782Z","iopub.execute_input":"2024-11-05T14:04:43.695253Z","iopub.status.idle":"2024-11-05T14:04:43.706995Z","shell.execute_reply.started":"2024-11-05T14:04:43.695202Z","shell.execute_reply":"2024-11-05T14:04:43.705017Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_model(model, epoch_nums, optimizer, loss, new_train_x, new_train_y, new_cv_x, new_cv_y):\n\n\n    # Set the number of epochs\n    epochs = epoch_nums\n\n    # Put data to target device\n    new_train_x, new_train_y = new_train_x.to(device), (new_train_y.type(torch.float)).to(device)\n    new_cv_x, new_cv_y = new_cv_x.to(device), (new_cv_y.type(torch.float)).to(device)\n\n    # Build training and evaluation loop\n    for epoch in range(epochs):\n        ### Training\n        my_model.train()\n\n        # 1. Forward pass (model outputs raw logits)\n        y_logits = my_model(new_train_x).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n        y_pred = torch.round(y_logits) # turn logits -> pred probs -> pred labls\n\n        # 2. Calculate loss/accuracy\n        # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n        #                y_train) \n        train_loss = loss(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                       new_train_y) \n        acc = accuracy_fn(y_true=new_train_y, \n                          y_pred=y_pred) \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backwards\n        train_loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        ### Testing\n        my_model.eval()\n        with torch.inference_mode():\n            # 1. Forward pass\n            cv_logits = my_model(new_cv_x).squeeze() \n            cv_pred = torch.round(cv_logits)\n            # 2. Caculate loss/accuracy\n            cv_loss = loss(cv_logits,\n                                new_cv_y)\n            cv_acc = accuracy_fn(y_true=new_cv_y,\n                                   y_pred=cv_pred)\n\n        # Print out what's happening every 10 epochs\n        if epoch % 100 == 0:\n            print(f\"Epoch: {epoch} | Loss: {train_loss:.5f}, Accuracy: {acc:.2f}% | CV loss: {cv_loss:.5f}, CV acc: {cv_acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.709636Z","iopub.execute_input":"2024-11-05T14:04:43.710092Z","iopub.status.idle":"2024-11-05T14:04:43.722857Z","shell.execute_reply.started":"2024-11-05T14:04:43.710047Z","shell.execute_reply":"2024-11-05T14:04:43.721270Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Let's go!!\n# Let's first train the model for first 500 epochs with a learning rate of 0.01 with the optimizer\ntrain_model(my_model, 500, optimizer, loss, new_train_x, new_train_y, new_cv_x, new_cv_y)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T14:04:43.724427Z","iopub.execute_input":"2024-11-05T14:04:43.724855Z","iopub.status.idle":"2024-11-05T14:11:16.175363Z","shell.execute_reply.started":"2024-11-05T14:04:43.724814Z","shell.execute_reply":"2024-11-05T14:11:16.174391Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch: 0 | Loss: 2.30239, Accuracy: 0.00% | CV loss: 2.29041, CV acc: 0.00%\nEpoch: 100 | Loss: 1.48912, Accuracy: 97.30% | CV loss: 1.50417, CV acc: 95.76%\nEpoch: 200 | Loss: 1.47613, Accuracy: 98.51% | CV loss: 1.49379, CV acc: 96.76%\nEpoch: 300 | Loss: 1.47353, Accuracy: 98.76% | CV loss: 1.49360, CV acc: 96.74%\nEpoch: 400 | Loss: 1.47270, Accuracy: 98.84% | CV loss: 1.49260, CV acc: 96.86%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Awesome Let's check the final loss and accuracy on the cross validation set:\ncv_logits = my_model(new_cv_x).squeeze() \ncv_pred = torch.round(cv_logits)\n\ncv_loss = loss(cv_logits, (new_cv_y.type(torch.float)).to(device))\ncv_acc = accuracy_fn(y_true=new_cv_y, y_pred=cv_pred)\n\nprint(f\"The final loss on the CV set is: {cv_loss:.5f}\\n\\nThe final accuracy on the CV set is: {cv_acc:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:11:16.176846Z","iopub.execute_input":"2024-11-05T14:11:16.177482Z","iopub.status.idle":"2024-11-05T14:11:16.208083Z","shell.execute_reply.started":"2024-11-05T14:11:16.177432Z","shell.execute_reply":"2024-11-05T14:11:16.206885Z"}},"outputs":[{"name":"stdout","text":"The final loss on the CV set is: 1.49290\n\nThe final accuracy on the CV set is: 96.84\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Great we achived an accurecy of 96.84% on the CV set","metadata":{}},{"cell_type":"code","source":"# Let's see how our model performs on the test set\n\ntest_logits = my_model(new_test_x).squeeze() \ntest_pred = torch.round(test_logits)\n\ntest_loss = loss(test_logits, (new_test_y.type(torch.float)).to(device))\ntest_acc = accuracy_fn(y_true=new_test_y, y_pred=test_pred)\n\nprint(f\"The final loss on the test set is: {test_loss:.5f}\\n\\nThe final accuracy on the test set is: {test_acc:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:11:16.209772Z","iopub.execute_input":"2024-11-05T14:11:16.210248Z","iopub.status.idle":"2024-11-05T14:11:16.240606Z","shell.execute_reply.started":"2024-11-05T14:11:16.210196Z","shell.execute_reply":"2024-11-05T14:11:16.239255Z"}},"outputs":[{"name":"stdout","text":"The final loss on the test set is: 1.48851\n\nThe final accuracy on the test set is: 97.26\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Awesome we achived an accurecy of 97.26% on the test set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\n\ntest_pred = torch.round(test_logits.detach()).cpu()\ntest_true = new_test_y.detach().cpu()  \n\n\ntest_pred_np = test_pred.numpy()\ntest_true_np = test_true.numpy()\n\n\nprecision = precision_score(test_true_np, test_pred_np, average='macro')\nrecall = recall_score(test_true_np, test_pred_np, average='macro')\n\nprint(f\"Precision: {precision:.5f}\\nRecall: {recall:.5f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:22:52.840629Z","iopub.execute_input":"2024-11-05T14:22:52.841177Z","iopub.status.idle":"2024-11-05T14:22:52.882169Z","shell.execute_reply.started":"2024-11-05T14:22:52.841104Z","shell.execute_reply":"2024-11-05T14:22:52.880755Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.97284\nRecall: 0.97248\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# We achived a precision of 97.28% and a recall of 97.24%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:23:42.794267Z","iopub.execute_input":"2024-11-05T14:23:42.794720Z","iopub.status.idle":"2024-11-05T14:23:42.799973Z","shell.execute_reply.started":"2024-11-05T14:23:42.794650Z","shell.execute_reply":"2024-11-05T14:23:42.798710Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}